
from fastapi import Depends, FastAPI,Request,File,UploadFile
from pydantic import BaseModel 
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from sqlalchemy.orm import Session
from fastapi import Depends
import logging
from langchain.prompts.chat import MessagesPlaceholder


import psycopg2
from langchain_core.prompts import ChatPromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
import logging

app = FastAPI()
import google.generativeai as genai
from dotenv import load_dotenv
import os

from sqlalchemy import create_engine, Column, Integer, String
from sqlalchemy.ext.declarative import declarative_base
from typing import List
# genai.configure(api_key="AIzaSyD1J0vqebuEq6W1SCXnJh4rCErVD13yn4s")


from pydantic import BaseModel
from fastapi import FastAPI, UploadFile, File
from sqlalchemy import create_engine, Column, Text
from sqlalchemy.orm import sessionmaker, declarative_base

DATABASE_URL = "postgresql://postgres:password@localhost:5432/chatbot"
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
        
class FileData(Base):
    __tablename__ = "file_data"  # double underscores
    id = Column(Integer, primary_key=True, index=True)
    name = Column(String, index=True)
    description = Column(String)

Base.metadata.create_all(bind=engine) 

from pydantic import BaseModel

# Define a Pydantic model
class FileDataCreate(BaseModel):
    name: str
    description: str

class FileDataResponse(BaseModel):
    id: int
    name: str
    description: str

    class Config:
        from_attributes = True # Allows Pydantic to read attributes from ORM models

app = FastAPI()

@app.on_event("startup")
def load_files_db():
    db = SessionLocal()
    try:
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        files = ["chat.txt", "sales.txt", "prompt.txt"]

        for file in files:
            filepath = os.path.join(BASE_DIR, file)
            if os.path.exists(filepath):
                with open(filepath, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read().strip()
                    existing_item = db.query(FileData).filter(FileData.name == file).first()
                    if existing_item:
                        existing_item.description = content
                    else:
                        db.add(FileData(name=file, description=content))
        db.commit()
        print("Files loaded into DB")
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        db.close()


load_dotenv()



app.add_middleware(
    CORSMiddleware,
    allow_origins= 
    "http://localhost:3000",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

filepath = "data.txt"
texts=''
with open(filepath, 'r', encoding="utf-8") as f:
        content = f.read().strip()
        texts=content

# print(texts)

store ={}
# chat_history=[]
@app.post("/api/chat")
async def chat(request:Request,db: Session = Depends(get_db)):
    data = await request.json()
    logging.info(data)
    prompt = data.get("prompt","")
    filename = data.get("filename","")
    systeminstructions = ""
    if not prompt:
        print("Prompt is not available")
    if filename:
        item = db.query(FileData).filter(FileData.name == filename).first()
        if item:
            systeminstructions = item.description
    try:
        def get_session_history(session_id:str)->BaseChatMessageHistory:
            if session_id not in store:
                store[session_id] = InMemoryChatMessageHistory()
            return store[session_id]
        
        model = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            api_key=os.getenv("Gemini_api"),
        )
        system_instruction=[systeminstructions,texts]
        from langchain_core.prompts import MessagesPlaceholder

        prompt_includes = ChatPromptTemplate.from_messages([
            ("system", system_instruction),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{message}"),
                ])

        chain = prompt_includes| model
        with_message_history = RunnableWithMessageHistory(
            chain,
            get_session_history,
            input_messages_key="message", 
            history_messages_key="history",
        )
        response = with_message_history.invoke(
            {"message": prompt},
            config={"configurable": {"session_id": "user123"}}
        )
        logging.info(response.text)
        return ({
             "message" :response.content
        })
    except Exception as e:
        print(f"Error: {e}")

@app.get("/api/prompts")
def read_items(db: Session = Depends(get_db)):
    items = db.query(FileData).all()
    return items