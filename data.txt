@app.post("/api/chat/")
async def chat(request: Request, db: Session = Depends(get_db)):
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity as tfidf_cosine
    import numpy as np
    from numpy.linalg import norm

    data = await request.json()
    prompt = data.get("prompt", "")
    filename = data.get("filename", "")
    retrieval_type = data.get("retrieval_type", "dense")  # New param

    if not prompt:
        return {"error": "Prompt is required."}

    # Initialize embeddings (dense)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")
    user_vector = embeddings.embed_query(prompt)

    # Fetch all chunks and embeddings
    items = db.query(ChunkEmbedding.chunk_text, ChunkEmbedding.embedding).all()
    if not items:
        return {"error": "No embedded chunks found in database."}

    chunk_texts = [chunk_text for chunk_text, _ in items]

    # --- Dense Similarity ---
    dense_similarities = []
    for chunk_text, emb_str in items:
        emb_vec = np.fromstring(emb_str.strip("[]"), sep=",")
        score = np.dot(user_vector, emb_vec) / (norm(user_vector) * norm(emb_vec)) if norm(user_vector) and norm(emb_vec) else 0
        dense_similarities.append(score)

    # --- Sparse Similarity using TF-IDF ---
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(chunk_texts)
    query_vec_sparse = vectorizer.transform([prompt])
    sparse_similarities = tfidf_cosine(query_vec_sparse, tfidf_matrix).flatten()

    # --- Retrieval Logic ---
    top_chunks = []

    if retrieval_type == "dense":
        top_indices = np.argsort(dense_similarities)[::-1][:3]
        top_chunks = [chunk_texts[i] for i in top_indices]

    elif retrieval_type == "sparse":
        top_indices = np.argsort(sparse_similarities)[::-1][:3]
        top_chunks = [chunk_texts[i] for i in top_indices]

    elif retrieval_type == "hybrid":
        combined_scores = [
            0.5 * dense + 0.5 * sparse
            for dense, sparse in zip(dense_similarities, sparse_similarities)
        ]
        top_indices = np.argsort(combined_scores)[::-1][:3]
        top_chunks = [chunk_texts[i] for i in top_indices]

    else:
        return {"error": "Invalid retrieval_type. Use 'dense', 'sparse', or 'hybrid'."}

    rag_context = "\n\n".join(top_chunks)

    # Add file-specific instruction if exists
    file_instruction = ""
    if filename:
        item = db.query(FileData).filter(FileData.name == filename).first()
        if item:
            file_instruction = item.description

    # Final system prompt
    system_instruction = f"{texts}\n\n{file_instruction}\n\n{rag_context}"

    # Set up memory and model
    def get_session_history(session_id: str) -> BaseChatMessageHistory:
        if session_id not in store:
            store[session_id] = InMemoryChatMessageHistory()
        return store[session_id]

    model = ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        api_key=os.getenv("GOOGLE_API_KEY"),
        temperature=0.3,
    )

    prompt_template = ChatPromptTemplate.from_messages([
        ("system", system_instruction),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{message}"),
    ])

    chain = prompt_template | model

    with_message_history = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="message",
        history_messages_key="history",
    )

    response = with_message_history.invoke(
        {"message": prompt},
        config={"configurable": {"session_id": "user123"}}
    )

    return {"response": response.content}
